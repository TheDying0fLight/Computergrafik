{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVHXIH2cpk3q"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [],
      "source": [
        "# %pip install -U diffusers\n",
        "# %pip install transformers scipy ftfy accelerate\n",
        "# %pip install numpy==1.24.1\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "# %pip install huggingface_hub==0.25\n",
        "# %pip install protobuf\n",
        "# %pip install sentencepiece\n",
        "# %pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxASjCJLpk31"
      },
      "outputs": [],
      "source": [
        "%pip freeze > requirements.txt\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from thumbnail_generator import gemini_prompts, openllm_prompts\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Wrayrspk32"
      },
      "source": [
        "## Prompt generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiCVj4WTpk34"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"dataset/first.json\", \"r\") as f:\n",
        "    videos = json.load(f)\n",
        "transcript = ' '.join([v[\"text\"] for v in videos[1][\"caption\"]])\n",
        "print(transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMMKBbZXpk35"
      },
      "outputs": [],
      "source": [
        "try: transcript\n",
        "except Exception: transcript = None\n",
        "prompts = openllm_prompts(transcript)\n",
        "pos_prompt = prompts[\"positive\"]\n",
        "try: neg_prompt = prompts[\"negative\"]\n",
        "except Exception: neg_prompt = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9chcdPufpk36"
      },
      "outputs": [],
      "source": [
        "print(len(pos_prompt))\n",
        "print(pos_prompt)\n",
        "print(neg_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOjW68cpk38"
      },
      "source": [
        "## Image generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9rgzko7pk38"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from thumbnail_generator import Diffuser\n",
        "\n",
        "diff = Diffuser()\n",
        "try:\n",
        "    token = open(\"token.txt\", \"r\").read()\n",
        "    huggingface_hub.login(token)\n",
        "except Exception as e: print(e)\n",
        "\n",
        "loras_sd1_5 = {\n",
        "  0: \"sd1.5/none\",\n",
        "  1: \"sd1.5/200x1600\",\n",
        "  2: \"sd1.5/200x8000\",\n",
        "}\n",
        "\n",
        "loras = {\n",
        "  0: \"sdxl/none\",\n",
        "  1: \"sdxl/1024x1024-800\",\n",
        "  2: \"sdxl/1344x768-800\",\n",
        "  3: \"sdxl/1344x768Gaming200\",\n",
        "  4: \"sdxl/1344x768x200min500\",\n",
        "  5: \"sdxl/1344x768x200x1600-blured\",\n",
        "  6: \"sdxl/1344x768x200x1600-textfiltered\",\n",
        "  7: \"sdxl/1344x768-200-1600-500\",\n",
        "  8: \"sdxl/1344x768-200-1600-500-batch1\",\n",
        "  9: \"sdxl/1344x768-200-1600-500-fp16\",\n",
        "  10: \"sdxl/1344x768-60-1600-500-textfiltered-moondream\",\n",
        "  11: \"sdxl/1344x768-200-1600-500-moondream\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPckLWhApk39"
      },
      "source": [
        "### Changing models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gal5k1eXpk3-"
      },
      "outputs": [],
      "source": [
        "diff.set_model(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rUF7ANOpk3_"
      },
      "outputs": [],
      "source": [
        "diff.set_model(\"stabilityai/stable-diffusion-xl-base-1.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E50U1Y8Opk4B"
      },
      "outputs": [],
      "source": [
        "pipe = diff.optimized_sd3pipeline(\"stabilityai/stable-diffusion-3.5-medium\")\n",
        "diff.set_model(pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYXf862ppk4D"
      },
      "outputs": [],
      "source": [
        "img = diff.generate(\"A disco anchor drawn in microsoft paint by a child\", batch_size=4)\n",
        "grid = diff.get_grid()\n",
        "display(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8muT3Eepk4D"
      },
      "source": [
        "### Generating images and grids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylscg48YYxfF"
      },
      "outputs": [],
      "source": [
        "from thumbnail_generator import Youtube\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import re\n",
        "import os\n",
        "\n",
        "def remove_sentences(str, text):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', str)\n",
        "    result = ' '.join(sentence for sentence in sentences if text not in sentence)\n",
        "    return result\n",
        "\n",
        "def test_lora(lora, amount=20, description = \"gemini\", overwrite=True, filter_text=False):\n",
        "    yt = Youtube(\"dataset/first\")\n",
        "    videos = yt.videos\n",
        "    vid = 0\n",
        "    if \"none\" not in lora: diff.pipe.load_lora_weights(f\"loras/{lora}.safetensors\")\n",
        "    path = f\"test/{description}/{lora}\"\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    while amount > 0 and len(videos) > vid:\n",
        "        v = videos[vid]\n",
        "        vid += 1\n",
        "        id = v[\"id\"]\n",
        "        image = Image.open(f\"dataset/first/{id}.jpeg\")\n",
        "        store_path = f\"{path}/{str(id)}.jpeg\"\n",
        "        if image.size[1] > 500: continue\n",
        "        if not overwrite and os.path.exists(store_path): continue\n",
        "        txt = v[\"thumbnail_descriptions\"][description]\n",
        "        if filter_text:\n",
        "            txt = remove_sentences(txt, \"text\")\n",
        "            txt = remove_sentences(txt, '\"')\n",
        "        prompt = f\"A thumbnail showing {txt}\"\n",
        "        print(prompt)\n",
        "        display(image)\n",
        "        print(f\"{amount} generated\")\n",
        "        diff.generate(prompt, batch_size=4, width=912, height=512, seed=42)\n",
        "        clear_output()\n",
        "        print(f\"{vid}/{amount}\")\n",
        "        print(prompt)\n",
        "        display(image)\n",
        "        display(diff.get_grid())\n",
        "        diff.get_grid().save(store_path)\n",
        "        amount -= 1\n",
        "\n",
        "    diff.pipe.unload_lora_weights()\n",
        "\n",
        "test_lora(loras[0], 20, description=\"moondream\", filter_text=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fBPfbm4pk4F"
      },
      "source": [
        "## Youtube data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6vyctp9pk4F"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from thumbnail_generator import Youtube, Description\n",
        "yt = Youtube(\"dataset/first\")\n",
        "videos = yt.videos\n",
        "api_key = \"AIzaSyAOz2kX5yf8Sd3M5JcmARXZoY2GECYpmxw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IefUmH61pk4F"
      },
      "outputs": [],
      "source": [
        "print(\"Getting Videos\")\n",
        "vds = yt.get_popular(api_key, amount=None)\n",
        "print(\"Getting Thumbnails\")\n",
        "new_thumbnails = yt.add_thumbnails(amount=None, show=True)\n",
        "print(\"Getting Transcripts\")\n",
        "new_transcripts = yt.add_transcripts(amount=None)\n",
        "yt.to_json()\n",
        "print(\"Generating Gemini Thumbnail Descriptions\")\n",
        "gemini = Description.gemini(api_key)\n",
        "yt.generate_thumbnail_descriptions(gemini, amount=None, hz=15, show=True, overwrite=False, keyname=\"gemini\")\n",
        "yt.to_json()\n",
        "print(\"Generating InternVl2 Thumbnail Descriptions\")\n",
        "internvl = Description.internvl()\n",
        "yt.generate_thumbnail_descriptions(internvl, show=True, keyname=\"internvl2\")\n",
        "yt.to_json()\n",
        "print(\"Generating Moondream Thumbnail Descriptions\")\n",
        "internvl = Description.moondream()\n",
        "yt.generate_thumbnail_descriptions(internvl, show=True, keyname=\"moondream\")\n",
        "yt.to_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2QD6DgZpk4G"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import clear_output\n",
        "# from thumbnail_generator import categories\n",
        "\n",
        "\n",
        "def crop_and_resize(image: Image.Image, target_size):\n",
        "    original_width, original_height = image.size\n",
        "    target_width, target_height = target_size\n",
        "\n",
        "    original_aspect = original_width / original_height\n",
        "    target_aspect = target_width / target_height\n",
        "\n",
        "    if original_aspect > target_aspect:\n",
        "        new_width = int(original_height * target_aspect)\n",
        "        offset = (original_width - new_width) // 2\n",
        "        image = image.crop((offset, 0, offset + new_width, original_height))\n",
        "    else:\n",
        "        new_height = int(original_width / target_aspect)\n",
        "        offset = (original_height - new_height) // 2\n",
        "        image = image.crop((0, offset, original_width, offset + new_height))\n",
        "\n",
        "    return image.resize(target_size)\n",
        "\n",
        "\n",
        "# data preprocessing\n",
        "def to_dataset(data_path, store_path, res=(1344, 768), category=None,\n",
        "               min_y_res=0, amount=200, description=\"gemini\", text_filter=False):\n",
        "    IMG_FORMAT = \"jpeg\"\n",
        "    Path(store_path + \"/val\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(f\"{data_path}.json\", \"r\") as f: videos = json.load(f)\n",
        "    if amount is None: amount = len(videos)\n",
        "\n",
        "    csv_path = f\"{store_path}/csv.csv\"\n",
        "    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"file_name\", \"caption\"])\n",
        "        yolo = YOLO(\"textAndLabels.pt\", \"detect\")\n",
        "        # [v for v in videos if category is None or v[\"snippet\"][\"categoryId\"] == str(category)]:\n",
        "        for idx, v in enumerate(videos):\n",
        "            try:\n",
        "                id = v[\"id\"]\n",
        "                txt = v[\"thumbnail_descriptions\"][description]\n",
        "                # cat_id = v[\"snippet\"][\"categoryId\"]\n",
        "                # txt = f\"A thumbnail of category {categories[int(cat_id)]} showing {txt}\"\n",
        "                txt = f\"A thumbnail showing {txt}\"\n",
        "\n",
        "                image_source = f\"{data_path}/{id}.{IMG_FORMAT}\"\n",
        "                image = Image.open(image_source).convert('RGB')\n",
        "                dest = f\"{store_path}/val/{id}\"\n",
        "                # add to train data only if it has a single line description\n",
        "                clear_output()\n",
        "                print(f\"{idx+1}/{len(videos)}\")\n",
        "                if image.size[1] >= min_y_res and amount > 0 and \"\\n\" not in txt[0:-2]:  # and '\"' not in txt:\n",
        "                    if not text_filter or yolo(image_source, classes=[1], conf=0.5, augment=True)[0].boxes.data.size()[0] == 0:\n",
        "                        amount -= 1\n",
        "                        dest = f\"{store_path}/{id}\"\n",
        "                        image = crop_and_resize(image, res)\n",
        "                image.save(f\"{dest}.{IMG_FORMAT}\")\n",
        "                # shutil.copyfile(image_source, image_dest)\n",
        "                with open(f\"{dest}.txt\", \"w\") as txt_file: txt_file.write(txt)\n",
        "                csv_writer.writerow([f\"{id}.{IMG_FORMAT}\", txt])\n",
        "            except Exception as e: print(f\"Error processing video {v.get('id', 'unknown')}: {e}\")\n",
        "\n",
        "\n",
        "# SDXL\n",
        "to_dataset(\"dataset/first\", \"training_data/5_transformed thumbnail\",\n",
        "           min_y_res=500, description=\"moondream\", text_filter=False, amount=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5SK5-Tgpk4G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "\n",
        "# Load CLIP model and processor\n",
        "model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name).eval().cuda()\n",
        "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Function to load an image\n",
        "def load_image(image_path):\n",
        "    return Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Function to generate CLIP embeddings for text and image\n",
        "def generate_clip_embeddings(image_path, text):\n",
        "    image = load_image(image_path)\n",
        "    inputs = clip_processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "\n",
        "    return image_embeds, text_embeds\n",
        "\n",
        "# Function to combine embeddings\n",
        "def combine_embeddings(image_embedding, text_embedding, method=\"concat\"):\n",
        "    if method == \"concat\": return torch.cat((image_embedding, text_embedding), dim=-1)  # Concatenate along last dimension\n",
        "    elif method == \"add\": return image_embedding + text_embedding  # Element-wise addition\n",
        "    elif method == \"mul\": return image_embedding * text_embedding  # Element-wise multiplication\n",
        "    elif method == \"mean\": return (image_embedding + text_embedding) / 2  # Average\n",
        "\n",
        "image_path = r\"dataset\\first\\_ON1tS4UHzY.jpeg\"  # Replace with your image file path\n",
        "text = \"A scenic view of a mountain with a clear sky.\"  # Replace with your text\n",
        "\n",
        "# Generate embeddings\n",
        "image_embeds, text_embeds = generate_clip_embeddings(image_path, text)\n",
        "\n",
        "# Combine embeddings\n",
        "combined_embedding = combine_embeddings(image_embeds, text_embeds, method=\"mean\")\n",
        "print(\"Combined Embedding Shape:\", combined_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlD89_rjpk4H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "# Load CLIP model and processor\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).eval().cuda()\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Load Stable Diffusion pipeline\n",
        "stable_diffusion_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
        "sd_pipeline = StableDiffusionPipeline.from_pretrained(stable_diffusion_model_name, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "# Function to load and preprocess an image\n",
        "def load_image(image_path): return Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Function to generate CLIP embeddings for text and image\n",
        "def generate_clip_embeddings(image_path, text):\n",
        "    image = load_image(image_path)\n",
        "    inputs = clip_processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "    return image_embeds, text_embeds\n",
        "\n",
        "# Function to combine embeddings\n",
        "def combine_embeddings(image_embedding, text_embedding, method=\"concat\"):\n",
        "    if method == \"concat\":\n",
        "        return torch.cat((image_embedding, text_embedding), dim=-1)\n",
        "    elif method == \"add\":\n",
        "        return image_embedding + text_embedding\n",
        "    elif method == \"mul\":\n",
        "        return image_embedding * text_embedding\n",
        "    elif method == \"mean\":\n",
        "        return (image_embedding + text_embedding) / 2\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "# Function to use combined embedding with Stable Diffusion\n",
        "def generate_image_from_clip_embedding(image_path, text, method=\"concat\", guidance_scale=7.5):\n",
        "    # Step 1: Generate CLIP embeddings\n",
        "    image_embeds, text_embeds = generate_clip_embeddings(image_path, text)\n",
        "\n",
        "    # Step 2: Combine the embeddings\n",
        "    combined_embedding = combine_embeddings(image_embeds, text_embeds, method=method).unsqueeze(0)\n",
        "    print(combined_embedding.shape)\n",
        "\n",
        "    # Step 3: Generate the image using the combined embedding as prompt_embeds\n",
        "    with torch.no_grad():\n",
        "        image = sd_pipeline(\n",
        "            prompt_embeds=combined_embedding,  # Pass combined embeddings here\n",
        "            num_inference_steps=50,\n",
        "            guidance_scale=guidance_scale,\n",
        "        ).images[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "image_path = r\"dataset\\first\\_ON1tS4UHzY.jpeg\"  # Path to your image\n",
        "text = \"A scenic view of a mountain with a clear sky.\"  # Text input\n",
        "\n",
        "# Generate the image\n",
        "generated_image = generate_image_from_clip_embedding(image_path, text, method=\"concat\")\n",
        "\n",
        "# Save or display the image\n",
        "generated_image.save(\"generated_image_with_clip_embedding.png\")\n",
        "generated_image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SD Prompt Generation"
      ],
      "metadata": {
        "id": "AzkYzCqPrUvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important installs for moondream loading\n",
        "#!pip install pyvips\n",
        "#!apt-get install -y libvips42"
      ],
      "metadata": {
        "id": "gRDN6heK7h4Y"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installs for using notebook on colab\n",
        "#!pip install compel python-youtube pytube youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OULsJtYRrMFH",
        "outputId": "63008f88-0215-446f-f892-e3e6016008e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting compel\n",
            "  Downloading compel-2.0.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting python-youtube\n",
            "  Downloading python_youtube-0.9.7-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: diffusers>=0.11 in /usr/local/lib/python3.11/dist-packages (from compel) (0.32.2)\n",
            "Requirement already satisfied: pyparsing~=3.0 in /usr/local/lib/python3.11/dist-packages (from compel) (3.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from compel) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers~=4.25 in /usr/local/lib/python3.11/dist-packages (from compel) (4.47.1)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.0 (from python-youtube)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting isodate<=0.7.2,>=0.6.0 (from python-youtube)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from python-youtube) (2.32.3)\n",
            "Collecting requests-oauthlib==1.3.0 (from python-youtube)\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib==1.3.0->python-youtube) (3.2.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.0->python-youtube)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.0->python-youtube)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (0.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (0.5.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.11->compel) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->python-youtube) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->python-youtube) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->python-youtube) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.24.0->python-youtube) (2024.12.14)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.25->compel) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.25->compel) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.25->compel) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.25->compel) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->compel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->compel) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->compel) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->compel) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->compel) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.0->python-youtube)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers>=0.11->compel) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->compel) (3.0.2)\n",
            "Downloading compel-2.0.3-py3-none-any.whl (30 kB)\n",
            "Downloading python_youtube-0.9.7-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pytube, mypy-extensions, marshmallow, isodate, youtube_transcript_api, typing-inspect, requests-oauthlib, dataclasses-json, python-youtube, compel\n",
            "  Attempting uninstall: requests-oauthlib\n",
            "    Found existing installation: requests-oauthlib 1.3.1\n",
            "    Uninstalling requests-oauthlib-1.3.1:\n",
            "      Successfully uninstalled requests-oauthlib-1.3.1\n",
            "Successfully installed compel-2.0.3 dataclasses-json-0.6.7 isodate-0.7.2 marshmallow-3.26.0 mypy-extensions-1.0.0 python-youtube-0.9.7 pytube-15.0.0 requests-oauthlib-1.3.0 typing-inspect-0.9.0 youtube_transcript_api-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some packages needed for colab\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "cko22x1F43p5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for colab\n",
        "# mount Line's google drive to get access to finetuned models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49QBDTA7d2la",
        "outputId": "38d5de76-612d-4a86-8014-0f06b697a150"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pevcv-z6pk4I",
        "outputId": "4267976e-505f-44dd-d67a-2eed7871accb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'positive': \"The image shows a close-up of a person's face, with a focused expression and a slight smile.\",\n",
              " 'negative': 'The text is not visible in the image provided.'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "#import json\n",
        "from thumbnail_generator import PromptGenerator, Youtube\n",
        "\n",
        "transcript = \"test transcript\"\n",
        "\n",
        "# using the moondream model (TECHNICALLY WORKING NOW, CHECK RESULTS)\n",
        "PromptGenerator.moondream(transcript, vid_path = \"a01c.mp4\")\n",
        "\n",
        "# using the moondream model, finetuned on moondream captions (LOADING IT IS NOT WORKING YET)\n",
        "#PromptGenerator.moondream(transcript, vid_path = \"a01c.mp4\", path = \"/content/drive/My Drive/moondream/checkpoints/moondream/moondream-ft/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}